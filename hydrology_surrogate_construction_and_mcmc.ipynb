{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constraining subglacial processes from surface velocity observations using surrogate-based Bayesian inference\n",
    "## Part 1 - Training an ensemble of neural networks\n",
    "\n",
    "In this notebook, we will illustrate the process of using Bayesian Bootstrap Aggregation (BayesBag) to train an ensemble of neural networks.  In this case, each ensemble member is one possible surrogate for the coupled hydrology-ice dynamics model described in the paper, mapping from a vector of 8 parameters to a velocity field.  We begin by importing both the parameters and the associated velocity fields computed by the physics model, which will act as training data for the surrogate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import utilities\n",
    "\n",
    "# Load velocity fields\n",
    "F_lin = pickle.load(open('data/F_prior.p','rb'))\n",
    "\n",
    "# Load model parameters \n",
    "X = pickle.load(open('data/X_prior.p','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The velocity fields have some bad simulations in them, so we filter out circumstances in which the model never ran past 12 years, and in which the max velocity was greater than 100km/a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (X[:,1]<1e5)*(X[:,3]>=12)\n",
    "\n",
    "F_lin = F_lin[p]\n",
    "X = X[p,6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we log transform the velocity fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = np.log10(F_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use pytorch to construct and train the neural networks.  To this end, we will move the physical model's parameters and (log-)speed fields to pytorch, and use the GPU if it's available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "m = X.shape[0]\n",
    "\n",
    "X = torch.from_numpy(X)\n",
    "F = torch.from_numpy(F)\n",
    "\n",
    "X = X.to(torch.float32)\n",
    "F = F.to(torch.float32)\n",
    "\n",
    "X = X.to(device)\n",
    "F = F.to(device)\n",
    "\n",
    "X_hat = torch.log10(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of our objective function is to weight by element area.  We will grab those values from a .vtu of an observed velocity field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_obs = utilities.VData('./data/u_observed.vtu')\n",
    "point_area = torch.tensor(u_obs.get_point_area(),dtype=torch.float,device=device)\n",
    "normed_area = point_area/point_area.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define a few functions and classes.  First, we will create a function that extracts eigenglaciers and constructs the matrix $\\hat{V}$, corresponding to the Dimensionality Reduction section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eigenglaciers(omegas,F,cutoff=0.999):\n",
    "    F_mean = (F*omegas).sum(axis=0)\n",
    "    F_bar = F - F_mean # Eq. 28\n",
    "    S = F_bar.T @ torch.diag(omegas.squeeze()) @ F_bar # Eq. 27\n",
    "    lamda, V = torch.eig(S,eigenvectors=True) # Eq. 26\n",
    "    lamda = lamda[:,0].squeeze()\n",
    "    \n",
    "    cutoff_index = torch.sum(torch.cumsum(lamda/lamda.sum(),0)<cutoff)\n",
    "    lamda_truncated = lamda.detach()[:cutoff_index]\n",
    "    V = V.detach()[:,:cutoff_index]\n",
    "    V_hat = V @ torch.diag(torch.sqrt(lamda_truncated)) # A slight departure from the paper: Vhat is the\n",
    "                                                        # eigenvectors scaled by the eigenvalue size.  This\n",
    "                                                        # has the effect of allowing the outputs of the neural\n",
    "                                                        # network to be O(1).  Otherwise, it doesn't make \n",
    "                                                        # any difference.\n",
    "    return V_hat, F_bar, F_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we define the architecture of the neural network to be used as a surrogate.  This corresponds to the architecture defined in Fig. 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Emulator(nn.Module):\n",
    "    def __init__(self,n_parameters,n_eigenglaciers,n_hidden_1,n_hidden_2,n_hidden_3,n_hidden_4,V_hat,F_mean):\n",
    "        super().__init__()\n",
    "        # Inputs to hidden layer linear transformation\n",
    "        self.l_1 = nn.Linear(n_parameters, n_hidden_1)\n",
    "        self.norm_1 = nn.LayerNorm(n_hidden_1)\n",
    "        self.dropout_1 = nn.Dropout(p=0.0)\n",
    "        self.l_2 = nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.norm_2 = nn.LayerNorm(n_hidden_2)\n",
    "        self.dropout_2 = nn.Dropout(p=0.5)\n",
    "        self.l_3 = nn.Linear(n_hidden_2, n_hidden_3)\n",
    "        self.norm_3 = nn.LayerNorm(n_hidden_3)\n",
    "        self.dropout_3 = nn.Dropout(p=0.5)\n",
    "        self.l_4 = nn.Linear(n_hidden_3, n_hidden_4)\n",
    "        self.norm_4 = nn.LayerNorm(n_hidden_3)\n",
    "        self.dropout_4 = nn.Dropout(p=0.5)\n",
    "        self.l_5 = nn.Linear(n_hidden_4, n_eigenglaciers)\n",
    "\n",
    "        self.V_hat = torch.nn.Parameter(V_hat,requires_grad=False)\n",
    "        self.F_mean = torch.nn.Parameter(F_mean,requires_grad=False)\n",
    "\n",
    "    def forward(self, x, add_mean=False):\n",
    "        # Pass the input tensor through each of our operations\n",
    "\n",
    "        a_1 = self.l_1(x)\n",
    "        a_1 = self.norm_1(a_1)\n",
    "        a_1 = self.dropout_1(a_1)\n",
    "        z_1 = torch.relu(a_1) \n",
    "\n",
    "        a_2 = self.l_2(z_1)\n",
    "        a_2 = self.norm_2(a_2)\n",
    "        a_2 = self.dropout_2(a_2)\n",
    "        z_2 = torch.relu(a_2) + z_1\n",
    "\n",
    "        a_3 = self.l_3(z_2)\n",
    "        a_3 = self.norm_3(a_3)\n",
    "        a_3 = self.dropout_3(a_3)\n",
    "        z_3 = torch.relu(a_3) + z_2\n",
    "        \n",
    "        a_4 = self.l_4(z_3)\n",
    "        a_4 = self.norm_3(a_4)\n",
    "        a_4 = self.dropout_3(a_4)\n",
    "        z_4 = torch.relu(a_4) + z_3\n",
    "        \n",
    "        z_5 = self.l_5(z_4)\n",
    "        if add_mean:\n",
    "            F_pred = z_5 @ self.V_hat.T + self.F_mean\n",
    "        else:\n",
    "            F_pred = z_5 @ self.V_hat.T\n",
    "\n",
    "        return F_pred "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, we create an optimization procedure that trains a model for a given set of instance weights ($\\omega_d$) and training data.  Optimization is performed using mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "def criterion_ae(F_pred,F_obs,omegas,area):\n",
    "    instance_misfit = torch.sum(torch.abs((F_pred - F_obs))**2*area,axis=1)\n",
    "    return torch.sum(instance_misfit*omegas.squeeze())\n",
    "\n",
    "def train_surrogate(e,X_train,F_train,omegas,area,batch_size=128,epochs=3000,eta_0=0.01,k=1000.):\n",
    "    \n",
    "    omegas_0 = torch.ones_like(omegas)/len(omegas)\n",
    "    training_data = TensorDataset(X_train,F_train,omegas)\n",
    "\n",
    "    batch_size = 128\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=training_data,\n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(e.parameters(),lr=eta_0,weight_decay=0.0)\n",
    "    \n",
    "    # Loop over the data\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over each subset of data\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = eta_0*(10**(-epoch/k))\n",
    "\n",
    "        for x,f,o in train_loader:\n",
    "            e.train()\n",
    "            # Zero out the optimizer's gradient buffer\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            f_pred = e(x)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion_ae(f_pred,f,o,area)\n",
    "\n",
    "            # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "            loss.backward()\n",
    "        \n",
    "            # Use the derivative information to update the parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "        e.eval()\n",
    "        F_train_pred = e(X_train)\n",
    "        # Make a prediction based on the model\n",
    "        loss_train = criterion_ae(F_train_pred,F_train,omegas,area)\n",
    "        # Make a prediction based on the model\n",
    "        loss_test = criterion_ae(F_train_pred,F_train,omegas_0,area)\n",
    "\n",
    "        # Print the epoch, the training loss, and the test set accuracy.\n",
    "        if epoch%10==0:\n",
    "            print(epoch,loss_train.item(),loss_test.item())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we put it all together: loop over the desired number of models, drawing random Bayesian bootstrap weights for each, training the surrogate, and saving the resulting models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import dirichlet\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "n_parameters = X_hat.shape[1]\n",
    "n_hidden_1 = 128\n",
    "n_hidden_2 = 128\n",
    "n_hidden_3 = 128\n",
    "n_hidden_4 = 128\n",
    "\n",
    "n_models = 3 #To reproduce the paper, this should be 50\n",
    "for model_index in range(n_models):\n",
    "    omegas = torch.tensor(dirichlet.rvs(np.ones(m)),dtype=torch.float,device=device).T\n",
    "\n",
    "    V_hat, F_bar, F_mean = get_eigenglaciers(omegas,F)\n",
    "    n_eigenglaciers = V_hat.shape[1]\n",
    "\n",
    "    e = Emulator(n_parameters,n_eigenglaciers,n_hidden_1,n_hidden_2,n_hidden_3,n_hidden_4,V_hat,F_mean)\n",
    "    e.to(device)\n",
    "\n",
    "    train_surrogate(e,X_hat,F_bar,omegas,normed_area,epochs=3000)\n",
    "    torch.save(e.state_dict(),'emulator_ensemble/emulator_{0:03d}.h5'.format(model_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - MCMC over the ensemble\n",
    "Now that a number of neural network surrogates have been trained on random subsets of high-fidelity model runs, we will perform Markov Chain Monte Carlo sampling over each of these surrogates.  The correct parameter distribution for the high-fidelity model will be approximated by concatenating the Markov Chains over all of the surrogates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the models trained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "n_models = 3 #To reproduce the paper, this should be 50\n",
    "\n",
    "for i in range(n_models):\n",
    "    state_dict = torch.load('emulator_ensemble/emulator_{0:03d}.h5'.format(i))\n",
    "    e = Emulator(state_dict['l_1.weight'].shape[1],state_dict['V_hat'].shape[1],n_hidden_1,n_hidden_2,n_hidden_3,n_hidden_4,state_dict['V_hat'],state_dict['F_mean'])\n",
    "    e.load_state_dict(state_dict)\n",
    "    e.to(device)\n",
    "    e.eval()\n",
    "    models.append(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in some relevant training data and ancillary values.  Convert observed velocities to speeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_obs = utilities.VData('./data/u_observed.vtu')\n",
    "v_obs = utilities.VData('./data/v_observed.vtu')\n",
    "H_obs = utilities.VData('./data/H_observed.vtu')\n",
    "\n",
    "H = torch.tensor(H_obs.u)\n",
    "H = H.to(torch.float32).to(device)\n",
    "\n",
    "U_obs = torch.tensor(((np.sqrt(u_obs.u**2 + v_obs.u**2))))\n",
    "U_obs = U_obs.to(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the likelihood model, which requires a parameterization of observational uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "D = torch.tensor(squareform(pdist(u_obs.x)),dtype=torch.float32,device=device)\n",
    "\n",
    "sigma2 = 10**2\n",
    "sigma_flow2 = 10**2\n",
    "alpha_cov = 1\n",
    "\n",
    "l_model = 4*torch.sqrt(H.unsqueeze(1) @ H.unsqueeze(0))\n",
    "Sigma_obs = sigma2*torch.eye(D.shape[0],device=device)\n",
    "Sigma_flow = sigma_flow2*(1 + D**2/(2*alpha_cov*l_model**2))**-alpha_cov\n",
    "Sigma = Sigma_obs + Sigma_flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the precision matrix (the inverse of equation 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 1./(1e4**2)\n",
    "K = torch.diag(point_area*rho)\n",
    "Tau = K @ torch.inverse(Sigma) @ K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the Beta prior distribution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta\n",
    "alpha_b = 3.0\n",
    "beta_b = 3.0\n",
    "\n",
    "X_min = X_hat.cpu().numpy().min(axis=0)-1e-3\n",
    "X_max = X_hat.cpu().numpy().max(axis=0)+1e-3\n",
    "\n",
    "X_prior = beta.rvs(alpha_b,beta_b,size=(10000,8))*(X_max - X_min) + X_min\n",
    "\n",
    "X_min = torch.tensor(X_min,dtype=torch.float32,device=device)\n",
    "X_max = torch.tensor(X_max,dtype=torch.float32,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a value that is proportional to the negative log-posterior distribution (The summands of equation 53).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def V(X):\n",
    "    U_pred = 10**m(X,add_mean=True)\n",
    "    \n",
    "    r = (U_pred - U_obs)\n",
    "    X_bar = (X - X_min)/(X_max - X_min)\n",
    "\n",
    "    L1 = -0.5*r @ Tau @ r\n",
    "    L2 = torch.sum((alpha_b-1)*torch.log(X_bar) + (beta_b-1)*torch.log(1-X_bar)) \n",
    "\n",
    "    return -(L1 + L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Metropolis-adjusted Langevin Algorithm to sample from the posterior distribution, which benefits from the availability of gradient and Hessian information.  Here, we compute these quantities (and some helpful additional ones) using automatic differentiation in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_like_gradient_and_hessian(V,X,eps=1e-2,compute_hessian=False):\n",
    "    log_pi = V(X)\n",
    "    if compute_hessian:\n",
    "        g = torch.autograd.grad(log_pi,X,retain_graph=True,create_graph=True)[0]\n",
    "        H = torch.stack([torch.autograd.grad(e,X,retain_graph=True)[0] for e in g])\n",
    "        lamda,Q = torch.eig(H,eigenvectors=True)\n",
    "        lamda_prime = torch.sqrt(lamda[:,0]**2 + eps)\n",
    "        lamda_prime_inv = 1./torch.sqrt(lamda[:,0]**2 + eps)\n",
    "        H = Q @ torch.diag(lamda_prime) @ Q.T\n",
    "        Hinv = Q @ torch.diag(lamda_prime_inv) @ Q.T\n",
    "        log_det_Hinv = torch.sum(torch.log(lamda_prime_inv))\n",
    "        return log_pi,g,H,Hinv,log_det_Hinv\n",
    "    else: \n",
    "        return log_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the sampler by first finding the Maximum A Posteriori parameter value, or MAP point.  We find the MAP point using gradient descent paired with a simple line search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_MAP(X,n_iters=50,print_interval=10):\n",
    "    print('***********************************************')\n",
    "    print('***********************************************')\n",
    "    print('Finding MAP point')\n",
    "    print('***********************************************')\n",
    "    print('***********************************************')\n",
    "    # Line search distances\n",
    "    alphas = np.logspace(-4,0,11)\n",
    "    # Find MAP point\n",
    "    for i in range(n_iters):\n",
    "        log_pi,g,H,Hinv,log_det_Hinv = get_log_like_gradient_and_hessian(V,X,compute_hessian=True)\n",
    "        p = Hinv @ -g\n",
    "        alpha_index = np.nanargmin([get_log_like_gradient_and_hessian(V,X + alpha*p,compute_hessian=False).detach().cpu().numpy() for alpha in alphas])\n",
    "        mu = X + alphas[alpha_index] * p \n",
    "        X.data = mu.data\n",
    "        if i%print_interval==0:\n",
    "            print('===============================================')\n",
    "            print('iter: {0:d}, ln(P): {1:6.1f}, curr. m: {2:4.4f},{3:4.2f},{4:4.2f},{5:4.2f},{6:4.2f},{7:4.2f},{8:4.2f},{9:4.2f}'.format(i,log_pi,*X.data.cpu().numpy()))\n",
    "            print('===============================================')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a good initial guess for the sampler discovered, we now implement the MALA algorithm.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sample(mu,cov,eps=1e-10):\n",
    "    L = torch.cholesky(cov + eps*torch.eye(cov.shape[0],device=device))\n",
    "    return mu + L @ torch.randn(L.shape[0],device=device)\n",
    "\n",
    "def get_proposal_likelihood(Y,mu,inverse_cov,log_det_cov):\n",
    "    return -0.5*log_det_cov - 0.5*(Y - mu) @ inverse_cov @ (Y-mu)\n",
    "\n",
    "def MALA_step(X,h,local_data=None):\n",
    "    if local_data is not None:\n",
    "        pass  \n",
    "    else:\n",
    "        local_data = get_log_like_gradient_and_hessian(V,X,compute_hessian=True)\n",
    "        \n",
    "    log_pi,g,H,Hinv,log_det_Hinv = local_data\n",
    "    \n",
    "    X_ = draw_sample(X,2*h*Hinv).detach()\n",
    "    X_.requires_grad=True\n",
    "    \n",
    "    log_pi_ = get_log_like_gradient_and_hessian(V,X_,compute_hessian=False)\n",
    "\n",
    "    logq = get_proposal_likelihood(X_,X,H/(2*h),log_det_Hinv)\n",
    "    logq_ = get_proposal_likelihood(X,X_,H/(2*h),log_det_Hinv)\n",
    "\n",
    "    log_alpha = (-log_pi_ + logq_ + log_pi - logq)\n",
    "    alpha = torch.exp(min(log_alpha,torch.tensor([0.],device=device)))\n",
    "    u = torch.rand(1,device=device)\n",
    "    if u <= alpha and log_alpha!=np.inf:\n",
    "        X.data = X_.data\n",
    "        local_data = get_log_like_gradient_and_hessian(V,X,compute_hessian=True)\n",
    "        s = 1\n",
    "    else:\n",
    "        s = 0\n",
    "    return X,local_data,s\n",
    "\n",
    "def MALA(X,n_iters=10001,h=0.1,acc_target=0.25,k=0.01,beta=0.99,sample_path='./samples/',model_index=0,save_interval=1000,print_interval=50):\n",
    "    print('***********************************************')\n",
    "    print('***********************************************')\n",
    "    print('Running Metropolis-Adjusted Langevin Algorithm for model index {0}'.format(model_index))\n",
    "    print('***********************************************')\n",
    "    print('***********************************************')\n",
    "    local_data = None\n",
    "    vars = []\n",
    "    acc = acc_target\n",
    "    for i in range(n_iters):\n",
    "        X,local_data,s = MALA_step(X,h,local_data=local_data)\n",
    "        vars.append(X.detach())\n",
    "        acc = beta*acc + (1-beta)*s\n",
    "        h = min(h*(1+k*np.sign(acc - acc_target)),h_max)\n",
    "        if i%print_interval==0:\n",
    "            print('===============================================')\n",
    "            print('sample: {0:d}, acc. rate: {1:4.2f}, log(P): {2:6.1f}'.format(i,acc,local_data[0].item()))\n",
    "            print('curr. m: {0:4.4f},{1:4.2f},{2:4.2f},{3:4.2f},{4:4.2f},{5:4.2f},{6:4.2f},{7:4.2f}'.format(*X.data.cpu().numpy()))\n",
    "            print('===============================================')\n",
    "          \n",
    "        if i%save_interval==0:\n",
    "            print('///////////////////////////////////////////////')\n",
    "            print('Saving samples for model {0:03d}'.format(model_index))\n",
    "            print('///////////////////////////////////////////////')\n",
    "            X_posterior = torch.stack(vars).cpu().numpy()\n",
    "            np.save(open(sample_path+'X_posterior_model_{0:03d}.npy'.format(model_index),'wb'),X_posterior)\n",
    "    X_posterior = torch.stack(vars).cpu().numpy()\n",
    "    return X_posterior       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run the MAP/MALA procedure for each surrogate in the bootstrapped ensemble, and save the resulting posterior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "for j,m in enumerate(models):\n",
    "    X = torch.tensor(X_prior[np.random.randint(X_prior.shape[0],size=5)].mean(axis=0),requires_grad=True,dtype=torch.float,device=device)\n",
    "    X = find_MAP(X)\n",
    "    # To reproduce the paper, n_iters should be 10^5\n",
    "    X_posterior = MALA(X,n_iters=10000,model_index=j,save_interval=1000,print_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Add plotting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
